<!DOCTYPE html>
<!-- saved from url=(0051)http://rockybean.info/2012/08/15/jafka_consumer_src -->
<html class="no-js"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
    <meta name="renderer" content="webkit">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <title>Jafka源码阅读之Consumer - Rockybean</title>

    <!-- 使用url函数转换相关路径 -->
    <link rel="stylesheet" href="./Jafka源码阅读之Consumer - Rockybean_files/normalize.min.css">
    <link rel="stylesheet" href="./Jafka源码阅读之Consumer - Rockybean_files/grid.css">
    <link rel="stylesheet" href="./Jafka源码阅读之Consumer - Rockybean_files/style.css">

    <!--[if lt IE 9]>
    <script src="//cdn.staticfile.org/html5shiv/r29/html5.min.js"></script>
    <script src="//cdn.staticfile.org/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <!-- 通过自有函数输出HTML头部信息 -->
    <meta name="description" content="本文将介绍Jafka中Consumer的源码框架，在之前consumer的使用教程中已经讲过，消息消费有同步和异步两种方式，我们针对这两种方式分别讲解其源码实现。">
<meta name="keywords" content="jafka">
<meta name="generator" content="Typecho 1.0/14.10.10">
<meta name="template" content="default">
<link rel="pingback" href="http://rockybean.info/action/xmlrpc">
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="http://rockybean.info/action/xmlrpc?rsd">
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="http://rockybean.info/action/xmlrpc?wlw">
<link rel="alternate" type="application/rss+xml" title="RSS 2.0" href="http://rockybean.info/feed/2012/08/15/jafka_consumer_src">
<link rel="alternate" type="application/rdf+xml" title="RSS 1.0" href="http://rockybean.info/feed/rss/2012/08/15/jafka_consumer_src">
<link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="http://rockybean.info/feed/atom/2012/08/15/jafka_consumer_src">
<script type="text/javascript">
(function () {
    window.TypechoComment = {
        dom : function (id) {
            return document.getElementById(id);
        },
    
        create : function (tag, attr) {
            var el = document.createElement(tag);
        
            for (var key in attr) {
                el.setAttribute(key, attr[key]);
            }
        
            return el;
        },

        reply : function (cid, coid) {
            var comment = this.dom(cid), parent = comment.parentNode,
                response = this.dom('respond-post-59'), input = this.dom('comment-parent'),
                form = 'form' == response.tagName ? response : response.getElementsByTagName('form')[0],
                textarea = response.getElementsByTagName('textarea')[0];

            if (null == input) {
                input = this.create('input', {
                    'type' : 'hidden',
                    'name' : 'parent',
                    'id'   : 'comment-parent'
                });

                form.appendChild(input);
            }

            input.setAttribute('value', coid);

            if (null == this.dom('comment-form-place-holder')) {
                var holder = this.create('div', {
                    'id' : 'comment-form-place-holder'
                });

                response.parentNode.insertBefore(holder, response);
            }

            comment.appendChild(response);
            this.dom('cancel-comment-reply-link').style.display = '';

            if (null != textarea && 'text' == textarea.name) {
                textarea.focus();
            }

            return false;
        },

        cancelReply : function () {
            var response = this.dom('respond-post-59'),
            holder = this.dom('comment-form-place-holder'), input = this.dom('comment-parent');

            if (null != input) {
                input.parentNode.removeChild(input);
            }

            if (null == holder) {
                return true;
            }

            this.dom('cancel-comment-reply-link').style.display = 'none';
            holder.parentNode.insertBefore(response, holder);
            return false;
        }
    };
})();
</script>
<script type="text/javascript">
(function () {
    var event = document.addEventListener ? {
        add: 'addEventListener',
        focus: 'focus',
        load: 'DOMContentLoaded'
    } : {
        add: 'attachEvent',
        focus: 'onfocus',
        load: 'onload'
    };

    document[event.add](event.load, function () {
        var r = document.getElementById('respond-post-59');

        if (null != r) {
            var forms = r.getElementsByTagName('form');
            if (forms.length > 0) {
                var f = forms[0], textarea = f.getElementsByTagName('textarea')[0], added = false;

                if (null != textarea && 'text' == textarea.name) {
                    textarea[event.add](event.focus, function () {
                        if (!added) {
                            var input = document.createElement('input');
                            input.type = 'hidden';
                            input.name = '_';
                            input.value = (function () {
    var _KQoW = ''///*'gQ'*/'gQ'
+//'hi'
'36'+//'l'
'acf'+''///*'gPX'*/'gPX'
+'c'//'h'
+//'I'
'1'+'6a2'//'z3y'
+//'J'
'f'+'E'//'E'
+//'nmv'
'nmv'+//'8A'
'a'+'e'//'uoG'
+//'BY'
'ca'+//'zO'
'4'+//'3'
'3cf'+'cf2'//'l'
+/* 'CF4'//'CF4' */''+//'KB'
'KB'+//'e3'
'e3'+'3'//'m0T'
+'d4'//'i04'
+//'UDU'
'0b4'+'1f'//'qty'
+'e9'//'7b'
, _7hqQp = [[11,12],[11,14],[22,24],[22,24]];
    
    for (var i = 0; i < _7hqQp.length; i ++) {
        _KQoW = _KQoW.substring(0, _7hqQp[i][0]) + _KQoW.substring(_7hqQp[i][1]);
    }

    return _KQoW;
})();

                            f.appendChild(input);
                            added = true;
                        }
                    });
                }
            }
        }
    });
})();
</script></head>
<body>
<!--[if lt IE 8]>
    <div class="browsehappy" role="dialog">当前网页 <strong>不支持</strong> 你正在使用的浏览器. 为了正常的访问, 请 <a href="http://browsehappy.com/">升级你的浏览器</a>.</div>
<![endif]-->

<header id="header" class="clearfix">
    <div class="container">
        <div class="row">
            <div class="site-name col-mb-12 col-9">
                <a id="logo" href="http://rockybean.info/">
                                        Rockybean                </a>
        	    <p class="description">Just Be Cool!!! 继往者，启来者，不过是个烂笔头！</p>
            </div>
            <div class="site-search col-3 kit-hidden-tb">
                <form id="search" method="post" action="http://rockybean.info/2012/08/15/" role="search">
                    <label for="s" class="sr-only">搜索关键字</label>
                    <input type="text" name="s" class="text" placeholder="输入关键字搜索">
                    <button type="submit" class="submit">搜索</button>
                </form>
            </div>
            <div class="col-mb-12">
                <nav id="nav-menu" class="clearfix" role="navigation">
                    <a href="http://rockybean.info/">首页</a>
                                                            <a href="http://rockybean.info/about.html" title="关于">关于</a>
                                    </nav>
            </div>
        </div><!-- end .row -->
    </div>
</header><!-- end #header -->
<div id="body">
    <div class="container">
        <div class="row">

    
    

<div class="col-mb-12 col-8" id="main" role="main">
    <article class="post" itemscope="" itemtype="http://schema.org/BlogPosting">
        <h1 class="post-title" itemprop="name headline"><a itemtype="url" href="http://rockybean.info/2012/08/15/jafka_consumer_src">Jafka源码阅读之Consumer</a></h1>
        <ul class="post-meta">
            <li itemprop="author" itemscope="" itemtype="http://schema.org/Person">作者: <a itemprop="name" href="http://rockybean.info/author/1/" rel="author">rockybean</a></li>
            <li>时间: <time datetime="2012-08-15T07:38:00+08:00" itemprop="datePublished">August 15, 2012</time></li>
            <li>分类: <a href="http://rockybean.info/category/source_code/">源码解析</a></li>
        </ul>
        <div class="post-content" itemprop="articleBody">
            <p>本文将介绍Jafka中Consumer的源码框架，在之前consumer的<a href="http://rockybean.info/2012/07/24/jafka-consumer/">使用教程</a>中已经讲过，消息消费有同步和异步两种方式，我们针对这两种方式分别讲解其源码实现。</p>

<!--more-->

<h2>同步消费源码实现</h2>

<p>同步消费的代码如下：</p>

<pre><code class="lang-java">SimpleConsumer consumer = new SimpleConsumer("127.0.0.1",9092);
long offset = 0;
//指定fetch请求的参数：topic partition offset maxSize
FetchRequest fetch = new FetchRequest("person",0,offset,1024*1024);
try {
    //获取消息数据集合
    ByteBufferMessageSet messageSet = consumer.fetch(fetch);
    //遍历消息数据集合
    for(MessageAndOffset messageAndOffset:messageSet){
    //从消息中获取代表实际数据的byte数组
    ByteBuffer buffer = messageAndOffset.message.payload();
    byte[] objBytes = new byte[buffer.remaining()];
    buffer.get(objBytes);
    //反序列化字节数组为对象
    ObjectInputStream ois = new ObjectInputStream(new ByteArrayInputStream(objBytes));
    Person tmpPerson = (Person)ois.readObject();
    System.out.println("person:"+tmpPerson.getName()+","+tmpPerson.getId());
    ois.close();
}
} catch (IOException e) {
    e.printStackTrace(); 
} catch (ClassNotFoundException e) {
    e.printStackTrace(); 
}
</code></pre>

<p>上述代码的逻辑很简单：</p>

<ul>
<li><p>初始化一个SimpleConsumer对象，传入broker的url和端口号。</p></li>
<li><p>创建一个FetchRequest，传入topic、partition、起始offset值和要抓取的最大字节数。</p></li>
<li><p>调用consumer的fetch方法，返回ByteBufferMessageSet，该类中封装了获取的消息。</p></li>
<li><p>遍历ByteBufferMessageSet中的消息，进行处理</p></li>
</ul>

<p>下面我们来看下consumer.fetch方法的时序图。</p>

<p><img src="./Jafka源码阅读之Consumer - Rockybean_files/687860037.png" alt="jafka_consumer_fetch"></p>

<p>我们结合时序图来说明下consumer fetch的过程。</p>

<ul>
<li>SimpleConsumer继承了SimpleOperation，consumer.fetch会调用其继承下来的send方法(<code>2</code>)，send方法的代码如下：</li>
</ul>

<pre><code class="lang-java">public KV&lt;Receive, ErrorMapping&gt; send(Request request) throws IOException {
        return new SimpleCommand(request).run();
    }
</code></pre>

<p>其创建了一个SimpleCommand对象(<code>2.1</code>)，并调用其run方法(<code>2.2</code>)，返回了一个&lt;key,value&gt;对，key是一个Receive对象，其中包含了取得的消息数据。我们来看下其run方法。</p>

<pre><code class="lang-java">public KV&lt;Receive, ErrorMapping&gt; run() throws IOException {
     synchronized (lock) {
     //建立到broker的连接
         getOrMakeConnection();
         try {
         //发送请求
             sendRequest(request);
             //获取返回值
             return getResponse();
         } catch (IOException e) {
             logger.info("Reconnect in fetch request due to socket error:", e);
             try {
                 channel = connect();
                 sendRequest(request);
                 return getResponse();
             } catch (IOException e2) {
                 throw e2;
             }
         }
         //
     }
}

</code></pre>

<p>上述代码的逻辑很简单：建立到broker的连接；发送请求；获取返回值。<code>getOrMakeConnection</code>在consumer和broker之间建立阻塞连接，这里就不呈现其源码了。我们来看下sendRequest的源码。</p>

<pre><code class="lang-java">protected void sendRequest(Request request) throws IOException {
        new BoundedByteBufferSend(request).writeCompletely(channel);
}

</code></pre>

<p>sendRequest方法新建了一个BoundedByteBufferSend对象，并调用其writeCompletely方法，将request的内容发送到了broker。关于send的知识，在<a href="http://rockybean.info/2012/08/03/jafka-message/">这里</a>讲过，就不再赘述了。至此，fetch的请求就发送出去了，那么来看getResponse的代码。</p>

<pre><code class="lang-java">protected KV&lt;Receive, ErrorMapping&gt; getResponse() throws IOException {
        BoundedByteBufferReceive response = new BoundedByteBufferReceive();
        response.readCompletely(channel);
        return new KV&lt;Receive, ErrorMapping&gt;(response, ErrorMapping.valueOf(response.buffer().getShort()));
}

</code></pre>

<p>getResponse首先创建了一个BoundedByteBufferReceive对象，然后调用其readCompletely方法，从channel中将收到的数据读取到response中，并作为key返回。fetch方法最终的返回值是一个ByteBufferMessageSet对象，它封装了<code>response.k.buffer()</code>,buffer()返回的便是broker传回给consumer的结果。至此，consumer的fetch过程也就结束了。</p>

<h2>异步消费源码实现</h2>

<p>同步消费是单线程阻塞完成的，从其代码调用上，我们可以看到其api封装比较靠近底层，对使用者不友好，一般没有特殊需求不推荐使用。异步消费封装了对用户友好的api，在同步消费的基础上实现了多线程消费数据的功能。我们先来理一下它的相关代码：</p>

<pre><code class="lang-java">Properties props = new Properties();
//指明zookeeper地址
props.setProperty("zk.connect","localhost:2181");
//指明consumer group的名字
props.setProperty("groupid","test_group");
ConsumerConfig config = new ConsumerConfig(props);
//创建了ZookeeperConsumerConnector，连接zookeeper，获取当前topic的数据信息
ConsumerConnector connector = Consumer.create(config);
//指明每一个topic的消费线程数
Map&lt;String,Integer&gt; topicCountMap = new HashMap&lt;String, Integer&gt;();
topicCountMap.put("hehe",2);
topicCountMap.put("hehe3",1);
//创建消费消息流，key为topic，value为MessageStream的list，大小为上面map中指定的大小
Map&lt;String,List&lt;MessageStream&lt;String&gt;&gt;&gt; streams = connector.createMessageStreams(topicCountMap,new StringDecoder());
List&lt;MessageStream&lt;String&gt;&gt; messageStreamList = streams.get("hehe");
messageStreamList.addAll(streams.get("hehe3"));
final AtomicInteger count = new AtomicInteger(0);
final AtomicInteger streamCount = new AtomicInteger(0);
//创建线程池，该线程池数目必须不小于上面所有的消费线程数
ExecutorService executor = Executors.newFixedThreadPool(3);
//提交消费任务，开始消费消息
for(final MessageStream&lt;String&gt; stream:messageStreamList){
executor.execute(new Runnable() {
    @Override
    public void run() {
        int threadNum = streamCount.incrementAndGet();
        //从stream中获取消息，此处为阻塞式消费，即当没有新消息到来时，阻塞直到新消息到来或者线程结束
        //通过BlockingQueue实现，后续内容会详细讲解
        for(String msg:stream){
        System.out.println("stream#"+threadNum+":msg#"+count.incrementAndGet()+"=&gt;"+msg);
        }
    }
});
}
try {
    executor.awaitTermination(1, TimeUnit.HOURS);
} catch (InterruptedException e) {
    e.printStackTrace();
}

</code></pre>

<p>其基本的流程如下：</p>

<ul>
<li><p>构建配置信息，将consumer的配置写入其中，这里配置了zookeeper的连接地址和consumer group的名字，这是最基本的配置。</p></li>
<li><p>根据配置创建ConsumerConnector，由名字可猜测该对象负责consumer连接zk、broker并获取数据的工作。这里使用的是其子类ZookeeperConsumerConnector。</p></li>
<li><p>指明要消费的topic以及并行消费的线程数目，可以指定多个topic。</p></li>
<li><p>使用connector来创建消息消费流(MessageStream)，这里<code>List&lt;MessageStream&gt;</code>的数目是由上面配置的线程数来决定的。</p></li>
<li><p>每一个MessageStream中包含了从broker传来的消息，遍历它便可以获取数据，进行自己的操作</p></li>
</ul>

<p>这个流程虽然有些复杂，但使用者只要了解了并行消费的特点，应该可以很好地使用。我们来看下其时序图：</p>

<p><img src="./Jafka源码阅读之Consumer - Rockybean_files/1059081944.png" alt="jafka_consumer_async"></p>

<p>是不是看的有些晕？没关系，我们来一步步地看。</p>

<ul>
<li>Consumer.create方法创建了一个ZookeeperConsumerConnector对象，其初始化函数如下：</li>
</ul>

<pre><code class="lang-java">public ZookeeperConsumerConnector(ConsumerConfig config) {
        this(config, true);
    }

public ZookeeperConsumerConnector(ConsumerConfig config, boolean enableFetcher) {
        this.config = config;
        this.enableFetcher = enableFetcher;
        //
        this.topicRegistry = new Pool&lt;String, Pool&lt;Partition, PartitionTopicInfo&gt;&gt;();
        this.queues = new Pool&lt;StringTuple, BlockingQueue&lt;FetchedDataChunk&gt;&gt;();
        //
        // 建立到zookeeper的连接
        connectZk();
        //创建一个Fetcher，用于抓取数据
        //todo:此处fetcher在consumer确定要消费的partition后，调用 fetcher.startConnection，开始连接到broker，抓取数据
        createFetcher();
        if (this.config.isAutoCommit()) {
            logger.info("starting auto committer every " + config.getAutoCommitIntervalMs() + " ms");
            //启动自动提交消费offset的线程
            scheduler.scheduleWithRate(new AutoCommitTask(), config.getAutoCommitIntervalMs(),
                    config.getAutoCommitIntervalMs());
        }
    }

</code></pre>

<p>这里有几个重要的变量介绍一下：</p>

<ul>
<li><p>topicRegistry:记录了该consumer消费的所有topic对应的partition，从中可以获取消费的当前offset等等（存储在PartitionTopicInfo中），本身是一个map结构，内容为&lt;topic,&lt;partition,partitionTopicInfo&gt;&gt;。</p></li>
<li><p>queues:StringTuple为一个二元组，这里就是&lt;String,String&gt;类型，queues也是一个map结构，其内容为&lt;(topic,threadId),dataInfoList)。其value中存储了该topic下的消费线程threadId从broker拉取到的数据，在实时抓取的应用场景下，当有消息到达broker后，该consumer便会间隔一段时间后将数据获取，填入dataInfoList，之后返回给前台用户来消费就可以了。</p></li>
</ul>

<p>该connector还建立了到zookeeper的连接，以备后面读取zk上注册的信息，又建立了一个fetcher，为之后建立到broker的连接做好准备。最后，如果用户配置了自动将消费的offset信息提交到zookeeper存储的话，这里会开启一个后台线程，定期地将本consumer的消费信息(offset值)保存到zookeeper。</p>

<ul>
<li>createMessageStreams是connector的关键函数，它负责创建consumer的多个消费线程，拉取数据。再讲解其源码之前，我们来考虑其实现的几个关键问题，然后带着这些问题去看源码，会大大提高效率。</li>
</ul>

<p>1.consumer连接了zookeeper，它从zookeeper获取了哪些信息？</p>

<p>我们可以先来猜测下，每个topic的具体数据是存放在各个broker中的，并且以broker-partition的形式存储在broker中，那么consumer要消费某个topic就一定要获取这个topic存放的broker，以及这个broker上该topic的broker-partition列表，另外该consumer还应该直到它所在consumer group下有哪些个consumer，保证自己分配到的broker-partition不与其他consumer冲突。总结一下：broker信息、要消费topic的所有broker-partition、同一组的其他consumer。实际上是否如此哪？让我们到源码中去验证。</p>

<p>2.consumer是如何获取自己可以请求数据的broker-partition列表的？或者说consumer group是如何分配某topic下所有的broker-partition给其下多个consumer的？</p>

<p>3.consumer获取自己的broker-partition列表后是何时建立到broker的连接，以及如何实时获取新数据的？</p>

<p>下面我们在源码中找答案。createMessageStreams调用了自身的consume方法，这个方法有些长，如下：</p>

<pre><code class="lang-java"> private &lt;T&gt; Map&lt;String, List&lt;MessageStream&lt;T&gt;&gt;&gt; consume(Map&lt;String, Integer&gt; topicCountMap, Decoder&lt;T&gt; decoder) {
        if (topicCountMap == null) {
            throw new IllegalArgumentException("topicCountMap is null");
        }
        //初始化zk上consumer相关的路径名称
        ZkGroupDirs dirs = new ZkGroupDirs(config.getGroupId());
        //&lt;topic,msgStreamList&gt;
        Map&lt;String, List&lt;MessageStream&lt;T&gt;&gt;&gt; ret = new HashMap&lt;String, List&lt;MessageStream&lt;T&gt;&gt;&gt;();
        String consumerUuid = config.getConsumerId();
        if (consumerUuid == null) {
            //自动生成consumerUuid=&gt; hostname-currenttime-uuid.sub(8)
            consumerUuid = generateConsumerId();
        }
        logger.info(format("create message stream by consumerid [%s] with groupid [%s]", consumerUuid,
                config.getGroupId()));
        //
        //consumerIdString =&gt; groupid_consumerid
        final String consumerIdString = config.getGroupId() + "_" + consumerUuid;
        final TopicCount topicCount = new TopicCount(consumerIdString, topicCountMap);
        for (Map.Entry&lt;String, Set&lt;String&gt;&gt; e : topicCount.getConsumerThreadIdsPerTopic().entrySet()) {
            final String topic = e.getKey();
            final Set&lt;String&gt; threadIdSet = e.getValue();
            final List&lt;MessageStream&lt;T&gt;&gt; streamList = new ArrayList&lt;MessageStream&lt;T&gt;&gt;();
            for (String threadId : threadIdSet) {
                LinkedBlockingQueue&lt;FetchedDataChunk&gt; stream = new LinkedBlockingQueue&lt;FetchedDataChunk&gt;(
                        config.getMaxQueuedChunks());
                queues.put(new StringTuple(topic, threadId), stream);
                streamList.add(new MessageStream&lt;T&gt;(topic, stream, config.getConsumerTimeoutMs(), decoder));
            }
            ret.put(topic, streamList);
            logger.debug("adding topic " + topic + " and stream to map.");
        }
        //
        //listener to consumer and partition changes
        ZKRebalancerListener&lt;T&gt; loadBalancerListener = new ZKRebalancerListener&lt;T&gt;(config.getGroupId(),
                consumerIdString, ret);
        this.rebalancerListeners.add(loadBalancerListener);
        loadBalancerListener.start();
        registerConsumerInZK(dirs, consumerIdString, topicCount);

        //register listener for session expired event
        zkClient.subscribeStateChanges(new ZKSessionExpireListener&lt;T&gt;(dirs, consumerIdString, topicCount,
                loadBalancerListener));
                //监控consumer的变化
        zkClient.subscribeChildChanges(dirs.consumerRegistryDir, loadBalancerListener);

        for (String topic : ret.keySet()) {
            //register on broker partition path changes
            final String partitionPath = ZkUtils.BrokerTopicsPath + "/" + topic;
            zkClient.subscribeChildChanges(partitionPath, loadBalancerListener);
        }

        loadBalancerListener.syncedRebalance();
        return ret;
    }

</code></pre>

<p>这个方法列出了异步消费的主干代码，我们来看看都做了什么：</p>

<ul>
<li><p>创建ZkGroupDirs对象，内部封装了zookeeper的几个路径，如：/consumers/groups/[groupid]等，方便后面调用。之后获取consumerId，</p></li>
<li><p>获取或者系统生成consumerId,一般交由系统生成，格式为groupId_hostname-currenttime-uuid.sub(8)(uuid为调用函数库生成的uuid)。</p></li>
<li><p>创建TopicCount对象，调用其getConsumerThreadIdsPerTopic方法会根据topicCountMap为每个topic及其线程数生成每个consumerThreadId的名称，规则就是consumerId后面加"-"和线程数，以此作为这些消费线程的唯一性标识。其返回的数据是map类型，诸如&lt;topic,[groupid_consumerid-0,groupid_consumerid-1,groupid_consumerid-2]&gt;这种形式。</p></li>
<li><p>遍历上述map数据，将每个topic的consumerThreadId列表转换为MessageStream，并添加返回值ret中。大家注意，MessageStream出现了，这是API中出现过的类，我们来简单看下它的源码。</p></li>
</ul>

<pre><code class="lang-java">public class MessageStream&lt;T&gt; implements Iterable&lt;T&gt; {

    final String topic;

    final BlockingQueue&lt;FetchedDataChunk&gt; queue;

    final int consumerTimeoutMs;

    final Decoder&lt;T&gt; decoder;

    private final ConsumerIterator&lt;T&gt; consumerIterator;

    public MessageStream(String topic, BlockingQueue&lt;FetchedDataChunk&gt; queue, int consumerTimeoutMs, Decoder&lt;T&gt; decoder) {
        super();
        this.topic = topic;
        this.queue = queue;
        this.consumerTimeoutMs = consumerTimeoutMs;
        this.decoder = decoder;
        this.consumerIterator = new ConsumerIterator&lt;T&gt;(topic, queue, consumerTimeoutMs, decoder);
    }

    public Iterator&lt;T&gt; iterator() {
        return consumerIterator;
    }

    /**
     * This method clears the queue being iterated during the consumer
     * rebalancing. This is mainly to reduce the number of duplicates
     * received by the consumer
     */
    public void clear() {
        consumerIterator.clearCurrentChunk();
    }
}


</code></pre>

<p>可以看到每一个MessageStream对应一个topic，还有自己的数据队列queue，而迭代的实现是通过ConsumerIterator，这个迭代器的实现就不细讲了，它的功能就是不断地从queue中取出数据，这里T就是我们发送消息时指定的数据类型，一般都是用String，比如我们上面举的例子。有了这个迭代器，便有了代码中<code>for(String msg:stream)</code>类似的代码。</p>

<p>MessageStream的实现是简单的，可是这最后的返回值都构造好了，我们上面提到的3个问题却还是没有答案，那我们就接着往下看吧。</p>

<ul>
<li><p>35-51行是与zookeeper有关的代码，创建了一个负载均衡的listener，将该consumer注册到zookeeper上，监听consumer和broker的变化，变化时触发负载均衡的listener。</p></li>
<li><p>后面调用了loadbalancer的syncedRebalance方法，然后就return ret了。那我们那3个问题的答案就只能在这个方法里了。我们来看下这个方法的代码：</p></li>
</ul>

<pre><code class="lang-java">public void syncedRebalance() {
            synchronized (rebalanceLock) {
                for (int i = 0; i &lt; config.getMaxRebalanceRetries(); i++) {
                    if (isShuttingDown.get()) {//do nothing while shutting down
                        return;
                    }
                    logger.info(format("[%s] rebalancing starting. try #%d", consumerIdString, i));
                    final long start = System.currentTimeMillis();
                    boolean done = false;
                    //读取所有的broker
                    Cluster cluster = ZkUtils.getCluster(zkClient);
                    try {
                        done = rebalance(cluster);
                    } catch (Exception e) {
                        logger.info("exception during rebalance ", e);
                    }
                    ...
            }
        }

        private boolean rebalance(Cluster cluster) {
            //以topic做key组织threadId---当前consumer
            Map&lt;String, Set&lt;String&gt;&gt; myTopicThreadIdsMap = ZkUtils.getTopicCount(zkClient, group, consumerIdString)
                    .getConsumerThreadIdsPerTopic();
            //以topic做key组织threadid---所有的consumer
            //这里的consumer应该是每一个thread，即线程级别的
            Map&lt;String, List&lt;String&gt;&gt; consumersPerTopicMap = ZkUtils.getConsumersPerTopic(zkClient, group);
            //topic的broker-partition列表
            Map&lt;String, List&lt;String&gt;&gt; brokerPartitionsPerTopicMap = ZkUtils.getPartitionsForTopics(zkClient,
                    myTopicThreadIdsMap.keySet());
             //关闭当前抓取线程，否则会造成消息重复消费
            closeFetchers(cluster, messagesStreams, myTopicThreadIdsMap);
            //释放该consumer对topic对应broker-partition的绑定，因为重新分配后，该broker-partition可能由其他consumer处理
            releasePartitionOwnership(topicRegistry);
            Map&lt;StringTuple, String&gt; partitionOwnershipDecision = new HashMap&lt;StringTuple, String&gt;();
            Pool&lt;String, Pool&lt;Partition, PartitionTopicInfo&gt;&gt; currentTopicRegistry = new Pool&lt;String, Pool&lt;Partition, PartitionTopicInfo&gt;&gt;();
            //遍历当前consumer的topic下面的所有topicThread
            for (Map.Entry&lt;String, Set&lt;String&gt;&gt; e : myTopicThreadIdsMap.entrySet()) {
                final String topic = e.getKey();
                currentTopicRegistry.put(topic, new Pool&lt;Partition, PartitionTopicInfo&gt;());
                ZkGroupTopicDirs topicDirs = new ZkGroupTopicDirs(group, topic);
                //消费该topic的所有consumer列表
                List&lt;String&gt; curConsumers = consumersPerTopicMap.get(topic);
                //该topic下所有的broker-partition列表
                List&lt;String&gt; curBrokerPartitions = brokerPartitionsPerTopicMap.get(topic);
                //计算每个consumer要分配的partition的数目
                final int nPartsPerConsumer = curBrokerPartitions.size() / curConsumers.size();
                //计算平均分配后多出的partition数目
                final int nConsumersWithExtraPart = curBrokerPartitions.size() % curConsumers.size();

                //consumerThreadId=&gt; groupid_consumerid-index (index from count)
                //遍历当前consumer的所有stream
                for (String consumerThreadId : e.getValue()) {
                    final int myConsumerPosition = curConsumers.indexOf(consumerThreadId);
                    assert (myConsumerPosition &gt;= 0);
                    final int startPart = nPartsPerConsumer * myConsumerPosition + Math.min(myConsumerPosition,
                            nConsumersWithExtraPart);
                    final int nParts = nPartsPerConsumer + ((myConsumerPosition + 1 &gt; nConsumersWithExtraPart) ? 0 : 1);

                    if (nParts &lt;= 0) {
                        logger.warn("No broker partitions consumed by consumer thread " + consumerThreadId + " for topic " + topic);
                    } else {
                        for (int i = startPart; i &lt; startPart + nParts; i++) {
                        //获取对应的broker-partition
                            String brokerPartition = curBrokerPartitions.get(i);
                            logger.info("[" + consumerThreadId + "] ==&gt; " + brokerPartition + " claimming");

                            addPartitionTopicInfo(currentTopicRegistry, topicDirs, brokerPartition, topic,
                                    consumerThreadId);
                            partitionOwnershipDecision.put(new StringTuple(topic, brokerPartition), consumerThreadId);
                        }
                    }
                }
            }
            //将任务分配的结果注册到zookeeper上
            if (reflectPartitionOwnershipDecision(partitionOwnershipDecision)) {
                logger.debug("Updating the cache");
                logger.debug("Partitions per topic cache " + brokerPartitionsPerTopicMap);
                logger.debug("Consumers per topic cache " + consumersPerTopicMap);
                //记录当前consumer每个topic下面的其消费的broker-partition的信息&lt;topic,&lt;broker-partition,offset...&gt;&gt;
                topicRegistry = currentTopicRegistry;
                //启动fetcher抓取信息
                updateFetcher(cluster, messagesStreams);
                return true;
            } else {
                return false;
            }
        }

</code></pre>

<p>这个方法有些长，中间省去了非关键代码，我们来看下它的流程。</p>

<ul>
<li><p>11行：ZkUtils.getCluster方法，从zookeeper中获取所有的broker信息，之后调用rebalance方法。</p></li>
<li><p>23行-24： ZkUtils.getTopicCount.getConsumerThreadIdsPerTopic方法，从zookeeper中获取之前注册的当前consumer下每个topic的消费threadId</p></li>
<li><p>27行： 从zookeeper上获取所有topic对应的consumerThreadId，即获取同group的其他consumer信息。</p></li>
<li><p>29-30行： 从zookeeper中获取每个topic的broker-partition列表,要注意的是此处getPartitionsForTopics方法中有<code>Collections.sort(partList);</code>，这就意味着该列表是有序的，其顺序如下：0-0,0-1,0-2,1-0,1-1,2-0,2-1;保证同broker的在也一起，这样分配的时候也能保证同一个broker的partition分配给一个consumer，减少连接数目。</p>

<p>至此，第一个问题显然已经解决了，与我们预料的基本相同。</p></li>
<li><p>接下来关闭当前所有的抓取线程，防止出现消息重复消费的情况。因为重新分配后，该broker-partition可能被其他consumer消费，而如果该fetcher不停的话，它依然会去消费该broker-partition的数据。之后释放该consumer关联的broker-partition。</p></li>
<li><p>38-74行：该循环即是负载均衡的主体，其实现理念可以参见前面consumer的<a href="http://rockybean.info/2012/07/24/jafka-consumer/">文章</a>，实现代码也清晰明了。这里简单说一下addPartitionTopicInfo方法，该方法将consumerThreadId与partition关联在一起，方法为构建一个PartitionTopiCInfo对象，该对象包含了topic partition queue(数据队列) offset等信息，offset的确定也是在该方法中进行的。如果是第一次读，根据用户的配置，是从最近的消息获取，还是从最早的消息获取，否则从上一次消费的位置继续消费。但这里也只是完成了分配，即第二个问题解决了，那取数据的操作是在哪里完成的？</p></li>
<li><p>76-83行：首先将分配的信息注册到zookeeper上，然后调用了<code>updateFetcher</code>方法，我们来看下它的源码。</p></li>
</ul>

<pre><code class="lang-java">private void updateFetcher(Cluster cluster, Map&lt;String, List&lt;MessageStream&lt;T&gt;&gt;&gt; messagesStreams2) {
     if (fetcher != null) {
         List&lt;PartitionTopicInfo&gt; allPartitionInfos = new ArrayList&lt;PartitionTopicInfo&gt;();
         for (Pool&lt;Partition, PartitionTopicInfo&gt; p : topicRegistry.values()) {
             allPartitionInfos.addAll(p.values());
         }
         //创立连接
         fetcher.startConnections(allPartitionInfos, cluster, messagesStreams2);
     }
}

</code></pre>

<p>该方法首先从topicRegistry中获取了该consumer下所有的PartitionTopicInfo对象，该对象中包含了拉取数据所需要的所有信息，如broker offset queue等等。之后调用fetch.startConnections方法。从名字上我们也知道，第三个问题的答案找到了，我们还是来看一下这个方法的源码吧！</p>

<pre><code class="lang-java"><br>public &lt;T&gt; void startConnections(Iterable&lt;PartitionTopicInfo&gt; topicInfos,Cluster cluster,//
            Map&lt;String,List&lt;MessageStream&lt;T&gt;&gt;&gt; messageStreams){
        if(topicInfos == null) {
            return;
        }

        //re-arrange by broker id
        //&lt;brokerid,partition&gt;
        Map&lt;Integer, List&lt;PartitionTopicInfo&gt;&gt; m = new HashMap&lt;Integer, List&lt;PartitionTopicInfo&gt;&gt;();
        for (PartitionTopicInfo info : topicInfos) {
            if (cluster.getBroker(info.brokerId) == null) {
                throw new IllegalStateException("Broker " + info.brokerId + " is unavailable, fetchers could not be started");
            }
            List&lt;PartitionTopicInfo&gt; list = m.get(info.brokerId);
            if (list == null) {
                list = new ArrayList&lt;PartitionTopicInfo&gt;();
                m.put(info.brokerId, list);
            }
            list.add(info);
        }
        //
        final List&lt;FetcherRunnable&gt; fetcherThreads = new ArrayList&lt;FetcherRunnable&gt;();
        //创建到所有broker的连接，启动线程，开始定时抓取数据
        for(Map.Entry&lt;Integer, List&lt;PartitionTopicInfo&gt;&gt; e:m.entrySet()) {
            FetcherRunnable fetcherThread = new FetcherRunnable("FetchRunnable-"+e.getKey(), //
                    zkClient, //
                    config, //
                    cluster.getBroker(e.getKey()), //
                    e.getValue());
            fetcherThreads.add(fetcherThread);
            fetcherThread.start();
        }
        //
        this.fetcherThreads = fetcherThreads;
    }

</code></pre>

<p>该方法主要两个步骤：按brokerId划分所有的PartitionTopicInfo对象和为每个broker开启一个线程来拉取数据。FetcherRunnable的代码这里就不贴了，感兴趣的读者可以自己去看，它将PartitionTopicInfo对象列表中的信息封装成一个MultiFetchRequest对象，然后调用SimpleConsumer的multiFetch方法来拉取数据，获得数据后将其添加到PartitionTopicInfo的queue中（也就添加到了MessageStream的queue中，这样前台就能立即消费数据了），之后更新其fetchOffSet等信息，继续抓取数据，如果抓取不到数据，则睡眠一段时间再去抓取，这样便实现了即时消费的功能。</p>

<p>至此三个问题都解决了，大家对consumer的代码框架应该也了然于胸了吧。</p>

<h2>小结</h2>

<p>本文主要讲解了jafka中同步消费和异步消费的源码实现，异步消费是建立在同步消费基础上的高等api，实现了并行消费消息的功能，其实现方式值得大家好好研究。本文只是尝试讲解其代码的脉络，希望对大家有所帮助。</p>
        </div>
        <p itemprop="keywords" class="tags">标签: <a href="http://rockybean.info/tag/jafka/">jafka</a></p>
    </article>

    <div id="comments">
        
        <div id="respond-post-59" class="respond">
        <div class="cancel-comment-reply">
        <a id="cancel-comment-reply-link" href="http://rockybean.info/2012/08/15/jafka_consumer_src#respond-post-59" rel="nofollow" style="display:none" onclick="return TypechoComment.cancelReply();">取消回复</a>        </div>
    
    	<h3 id="response">添加新评论</h3>
    	<form method="post" action="http://rockybean.info/2012/08/15/jafka_consumer_src/comment" id="comment-form" role="form">
                		<p>
                <label for="author" class="required">称呼</label>
    			<input type="text" name="author" id="author" class="text" value="" required="">
    		</p>
    		<p>
                <label for="mail" class="required">Email</label>
    			<input type="email" name="mail" id="mail" class="text" value="" required="">
    		</p>
    		<p>
                <label for="url">网站</label>
    			<input type="url" name="url" id="url" class="text" placeholder="http://" value="">
    		</p>
                		<p>
                <label for="textarea" class="required">内容</label>
                <textarea rows="8" cols="50" name="text" id="textarea" class="textarea" required=""></textarea>
            </p>
    		<p>
                <button type="submit" class="submit">提交评论</button>
            </p>
    	</form>
    </div>
    </div>

    <ul class="post-near">
        <li>上一篇: <a href="http://rockybean.info/2012/08/14/jafka_producer_src" title="Jafka源码阅读之Producer">Jafka源码阅读之Producer</a></li>
        <li>下一篇: <a href="http://rockybean.info/2012/08/20/jafka_distributed" title="Jafka分布式特性研究">Jafka分布式特性研究</a></li>
    </ul>
</div><!-- end #main-->

<div class="col-mb-12 col-offset-1 col-3 kit-hidden-tb" id="secondary" role="complementary">
        <section class="widget">
		<h3 class="widget-title">最新文章</h3>
        <ul class="widget-list">
            <li><a href="http://rockybean.info/2016/12/05/180">crontab不能运行的问题</a></li><li><a href="http://rockybean.info/2016/05/24/under_gethostname">gethostname的背后机制</a></li><li><a href="http://rockybean.info/2015/04/13/ssh-login-debug-method-and-problems">ssh登录的调试方法和常见问题</a></li><li><a href="http://rockybean.info/2015/02/27/elasticsearch-date-type">Elasticsearch Date类型使用技巧</a></li><li><a href="http://rockybean.info/2015/02/25/kibana4-nginx-reverse-proxy">Kibana4使用nginx作反向代理</a></li><li><a href="http://rockybean.info/2015/02/09/elasticsearch-immense-term-exception">ElasticSearch immense term错误</a></li><li><a href="http://rockybean.info/2015/02/01/idea-integrate-xdebug">IDEA集成xdebug远程调试功能</a></li><li><a href="http://rockybean.info/2015/01/19/tar_chown_extraced_file">一次由tar解压引起的ssh登录失败事故</a></li><li><a href="http://rockybean.info/2015/01/02/pdu">名词解释之PDU</a></li><li><a href="http://rockybean.info/2014/11/29/tcp_self_connection">tcp自连接问题</a></li>        </ul>
    </section>
    
        <section class="widget">
		<h3 class="widget-title">最近回复</h3>
        <ul class="widget-list">
                            <li><a href="http://rockybean.info/2012/08/01/jafka-src#comment-789">xjk</a>: 顶一个</li>
                    <li><a href="http://rockybean.info/about.html#comment-788">adfa</a>: jafka是啥，只听说过kafka</li>
                    <li><a href="http://rockybean.info/2012/08/02/jafka-broker-socketserver#comment-787">chinesejie</a>: 这段socketServer 代码 跟cobar 中间件的代码 ...</li>
                    <li><a href="http://rockybean.info/2012/07/24/jafka-consumer#comment-786">chinesejie</a>: sorry 这段没错。</li>
                    <li><a href="http://rockybean.info/2012/07/24/jafka-consumer#comment-785">chinesejie</a>: 比如1024其实的消息数据为90B，其后的消息数据为30B，这两...</li>
                    <li><a href="http://rockybean.info/2012/07/24/jafka-consumer#comment-784">chinesejie</a>: 如果是一个broker集群，那么可能在一个broker上是tes...</li>
                    <li><a href="http://rockybean.info/about.html#comment-783">yy</a>: 非常感谢jafka相关内容的分享！</li>
                    <li><a href="http://rockybean.info/2015/02/27/elasticsearch-date-type#comment-781">kenshin23333</a>: 好用！感谢楼主！</li>
                    <li><a href="http://rockybean.info/2012/08/03/jafka-message#comment-780">宋鑫</a>: 好文要顶</li>
                    <li><a href="http://rockybean.info/2012/08/02/jafka-broker-socketserver#comment-779">宋鑫</a>: 可以考虑同步转载到CSDN，要不好像没什么人看</li>
                </ul>
    </section>
    
        <section class="widget">
		<h3 class="widget-title">分类</h3>
        <ul class="widget-list"><li class="category-level-0 category-parent"><a href="http://rockybean.info/category/default/">默认分类</a></li><li class="category-level-0 category-parent"><a href="http://rockybean.info/category/tech/">技术</a><ul class="widget-list"><li class="category-level-1 category-child category-level-odd"><a href="http://rockybean.info/category/network/">网络</a></li><li class="category-level-1 category-child category-level-odd"><a href="http://rockybean.info/category/algorithm/">算法</a></li><li class="category-level-1 category-child category-level-odd"><a href="http://rockybean.info/category/devops/">运维</a></li><li class="category-level-1 category-child category-level-odd"><a href="http://rockybean.info/category/language/">语言</a></li><li class="category-level-1 category-child category-level-odd"><a href="http://rockybean.info/category/source_code/">源码解析</a></li><li class="category-level-1 category-child category-level-odd"><a href="http://rockybean.info/category/database/">数据库</a></li><li class="category-level-1 category-child category-level-odd"><a href="http://rockybean.info/category/bug/">BUG</a></li><li class="category-level-1 category-child category-level-odd"><a href="http://rockybean.info/category/tools/">七种武器</a></li><li class="category-level-1 category-child category-level-odd"><a href="http://rockybean.info/category/front_end/">前端技术</a></li><li class="category-level-1 category-child category-level-odd"><a href="http://rockybean.info/category/optimize/">优化</a></li></ul></li><li class="category-level-0 category-parent"><a href="http://rockybean.info/category/book/">读书</a><ul class="widget-list"><li class="category-level-1 category-child category-level-odd"><a href="http://rockybean.info/category/tech_book/">技术这档子事</a></li><li class="category-level-1 category-child category-level-odd"><a href="http://rockybean.info/category/art_book/">文艺那档子事</a></li></ul></li><li class="category-level-0 category-parent"><a href="http://rockybean.info/category/mac/">mac大法</a></li><li class="category-level-0 category-parent"><a href="http://rockybean.info/category/talk/">随便扯扯</a></li></ul>	</section>
    
        <section class="widget">
		<h3 class="widget-title">归档</h3>
        <ul class="widget-list">
            <li><a href="http://rockybean.info/2016/12/">December 2016</a></li><li><a href="http://rockybean.info/2016/05/">May 2016</a></li><li><a href="http://rockybean.info/2015/04/">April 2015</a></li><li><a href="http://rockybean.info/2015/02/">February 2015</a></li><li><a href="http://rockybean.info/2015/01/">January 2015</a></li><li><a href="http://rockybean.info/2014/11/">November 2014</a></li><li><a href="http://rockybean.info/2014/10/">October 2014</a></li><li><a href="http://rockybean.info/2014/08/">August 2014</a></li><li><a href="http://rockybean.info/2014/07/">July 2014</a></li><li><a href="http://rockybean.info/2014/04/">April 2014</a></li><li><a href="http://rockybean.info/2013/11/">November 2013</a></li><li><a href="http://rockybean.info/2012/08/">August 2012</a></li><li><a href="http://rockybean.info/2012/07/">July 2012</a></li>        </ul>
	</section>
    
    	<section class="widget">
		<h3 class="widget-title">其它</h3>
        <ul class="widget-list">
                            <li class="last"><a href="http://rockybean.info/admin/login.php">登录</a></li>
                        <li><a href="http://rockybean.info/feed/">文章 RSS</a></li>
            <li><a href="http://rockybean.info/feed/comments/">评论 RSS</a></li>
            <li><a href="http://www.typecho.org/">Typecho</a></li>
        </ul>
	</section>
    <section class="widget">
	<ul class="widet-list" style="margin-left:99999px;">
<li><a href="http://www.cdliqing.com/">创大沥青</a></li>
</ul>
</section>

</div><!-- end #sidebar -->

        </div><!-- end .row -->
    </div>
</div><!-- end #body -->

<footer id="footer" role="contentinfo">
    © 2017 <a href="http://rockybean.info/">Rockybean</a>.
    由 <a href="http://www.typecho.org/">Typecho</a> 强力驱动.
</footer><!-- end #footer -->


<script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3F910e4fd2443eac0ca71a16d63ba69db5' type='text/javascript'%3E%3C/script%3E"));
</script><script src="./Jafka源码阅读之Consumer - Rockybean_files/h.js.下载" type="text/javascript"></script><a href="http://tongji.baidu.com/hm-web/welcome/ico?s=910e4fd2443eac0ca71a16d63ba69db5" target="_blank"><img border="0" src="./Jafka源码阅读之Consumer - Rockybean_files/21.gif" width="20" height="20"></a>




</body></html>